---
layout: distill
title: "ç”¨ JAX ç¼–ç¨‹ TPU"
# permalink: /main/
description: "æ‰‹æŠŠæ‰‹æ•™ä½ ç”¨ JAX æ“æ§ TPUï¼æœ¬èŠ‚å¤§éƒ¨åˆ†å†…å®¹å‚è€ƒè‡ª<a href='https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html'>å®˜æ–¹æ–‡æ¡£</a>ã€‚ä½ å¯ä»¥åœ¨ <a href='https://colab.sandbox.google.com/'>Google Colab</a> ä¸Šç™½å«–å…è´¹ TPU æ¥è·‘è¿™äº›ä»£ç ã€‚"
date: 2025-02-04
future: true
htmlwidgets: true
hidden: false

section_number: 10

previous_section_url: "../profiling"
previous_section_name: "ç¬¬9éƒ¨åˆ†ï¼šæ€§èƒ½åˆ†æ"

next_section_url: ../conclusion
next_section_name: "ç¬¬11éƒ¨åˆ†ï¼šç»“è®º"

giscus_comments: true

authors:
  - name: Jacob Austin
    url: "https://www.jacobaustin.org/"
    affiliations:
      name: Google DeepMind
  - name: Sholto Douglas
    url: "https://x.com/_sholtodouglas"
  - name: Roy Frostig
    url: "https://cs.stanford.edu/~rfrostig/"
  - name: Anselm Levskaya
    url: "https://anselmlevskaya.com/"
  - name: Charlie Chen
    url: "https://x.com/charliexychen"
  - name: Sharad Vikram
    url: "https://sharadvikram.com/"
  - name: Federico Lebron
    url: "https://fedelebron.com/"
  - name: Peter Choy
    url: "https://x.com/pchoy95"
  - name: Vinay Ramasesh
    url: "https://x.com/vinayramasesh"
  - name: Albert Webson
    url: "https://representation.ai/"
  - name: Yash Katariya
    url: https://x.com/yashk2810
  - name: Reiner Pope<sup>*</sup>
    url: https://x.com/reinerpope

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: "JAX å¹¶è¡Œç¼–ç¨‹ä¸‰æ¿æ–§"
  - subsections:
    - name: "è‡ªåŠ¨æŒ¡ï¼šè®©ç¼–è¯‘å™¨å¸®ä½ æå®š"
    - name: "åŠè‡ªåŠ¨æŒ¡ï¼šJAX æ§åˆ¶åˆ†ç‰‡ä¼ æ’­"
    - name: "æ‰‹åŠ¨æŒ¡ï¼šshard_map å…¨æ‰‹å†™"
  - name: "ç»ƒä¹ é¢˜"

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## JAX å¹¶è¡Œç¼–ç¨‹ä¸‰æ¿æ–§

> **ä¸€å¥è¯æ€»ç»“**ï¼šJAX æä¾›ä¸‰ç§å¹¶è¡Œç¼–ç¨‹æ¨¡å¼â€”â€”ä½ å¯ä»¥å®Œå…¨ä¿¡ä»»ç¼–è¯‘å™¨ã€åŠä¿¡ä»»è®© JAX ç®¡åˆ†ç‰‡ã€æˆ–è€…å…¨æ‰‹åŠ¨è‡ªå·±å†™é€šä¿¡ã€‚é€‰å“ªä¸ªå–å†³äºä½ æƒ³çœå¿ƒè¿˜æ˜¯æƒ³ç²¾ç»†æ§åˆ¶ã€‚

**å…ˆæ‰“ä¸ªæ¯”æ–¹**ï¼šå‡è®¾ä½ è¦æŒ‡æŒ¥ä¸€ä¸ª 8 äººä¹é˜Ÿæ¼”å¥äº¤å“ä¹ã€‚

| æ¨¡å¼ | æ¯”å–» | ä½ çš„å·¥ä½œé‡ |
|:---:|:---|:---:|
| **è‡ªåŠ¨æ¨¡å¼** | è¯·ä¸€ä¸ªæŒ‡æŒ¥å®¶ï¼Œä½ åªç®¡å†™è°±å­ | æœ€å°‘ |
| **æ˜¾å¼æ¨¡å¼** | ä½ æ˜¯æŒ‡æŒ¥ï¼Œä½†ä¹æ‰‹è‡ªå·±çœ‹è°±åè°ƒ | ä¸­ç­‰ |
| **æ‰‹åŠ¨æ¨¡å¼** | ä½ äº²è‡ªæŒ‡æŒ¥æ¯ä¸ªä¹æ‰‹çš„æ¯ä¸ªåŠ¨ä½œ | æœ€å¤š |

æ›´æŠ€æœ¯ä¸€ç‚¹è¯´ï¼ŒJAX æ”¯æŒä¸‰ç§æ€æƒ³æµæ´¾ï¼š

**1. ğŸ¤– è‡ªåŠ¨æŒ¡ï¼š"ç¼–è¯‘å™¨ï¼Œä½ æ¥æŒèˆµï¼"**
- æŠŠå•æœºä»£ç ç›´æ¥æ‰”ç»™ XLA ç¼–è¯‘å™¨
- ç¼–è¯‘å™¨è‡ªåŠ¨å†³å®šæ€ä¹ˆåˆ‡åˆ†æ•°æ®ã€åŠ ä»€ä¹ˆé€šä¿¡
- å¥½å¤„ï¼šä»£ç é›¶æ”¹åŠ¨å°±èƒ½è·‘åœ¨ 1000 å¼ å¡ä¸Š
- åå¤„ï¼šç¼–è¯‘å™¨æœ‰æ—¶å€™ä¼š"æŠ½é£"ï¼ŒåŠ ä¸€äº›è«åå…¶å¦™çš„é€šä¿¡

**2. ğŸš— åŠè‡ªåŠ¨æŒ¡ï¼š"JAXï¼Œå¸®æˆ‘ç›¯ç€ï¼"**
- ä½ å†™å•æœºä»£ç ï¼ŒJAX è´Ÿè´£ä¼ æ’­åˆ†ç‰‡ä¿¡æ¯
- åˆ†ç‰‡ä¿¡æ¯å˜æˆç±»å‹ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†
- é‡åˆ°æ¨¡ç³Šæƒ…å†µï¼ŒJAX ä¼šæŠ¥é”™è®©ä½ æ˜ç¡®
- æ¯”è‡ªåŠ¨æŒ¡å¯æ§ï¼Œæ¯”æ‰‹åŠ¨æŒ¡çœäº‹

**3. ğŸï¸ æ‰‹åŠ¨æŒ¡ï¼š"è€å­è‡ªå·±æ¥ï¼"**
- ä½ æ‹¿åˆ°çš„æ˜¯æ¯ä¸ªè®¾å¤‡çš„æœ¬åœ°è§†å›¾
- æ‰€æœ‰é€šä¿¡ï¼ˆAllGatherã€AllReduce ç­‰ï¼‰è‡ªå·±å†™
- å®Œå…¨æŒæ§ï¼Œä½†å†™èµ·æ¥è´¹åŠ²
- é€‚åˆæ€§èƒ½æè‡´ä¼˜åŒ–åœºæ™¯

| æ¨¡å¼ | ä½ çœ‹åˆ°çš„è§†å›¾ | éœ€è¦æŒ‡å®šåˆ†ç‰‡ï¼Ÿ | éœ€è¦å†™é€šä¿¡ï¼Ÿ |
|:---:|:---:|:---:|:---:|
| è‡ªåŠ¨ | å…¨å±€ï¼ˆæ•´ä¸ªæ•°ç»„ï¼‰ | âŒ | âŒ |
| æ˜¾å¼ | å…¨å±€ï¼ˆæ•´ä¸ªæ•°ç»„ï¼‰ | âœ… | âŒ |
| æ‰‹åŠ¨ | æœ¬åœ°ï¼ˆå½“å‰è®¾å¤‡çš„é‚£å—ï¼‰ | âœ… | âœ… |

å¯¹åº”çš„ JAX APIï¼š

| æ¨¡å¼ | API | ç‰¹ç‚¹ |
|:---|:---|:---|
| è‡ªåŠ¨ | `jax.jit` + Auto mesh | XLA [Shardy](https://openxla.org/shardy) è‡ªåŠ¨åŠ é€šä¿¡ |
| æ˜¾å¼ | `jax.jit` + Explicit mesh | JAX è¿½è¸ªåˆ†ç‰‡ï¼Œé‡åˆ°æ­§ä¹‰æŠ¥é”™ |
| æ‰‹åŠ¨ | `jax.shard_map` | æœ¬åœ°è§†å›¾ï¼Œæ‰‹å†™ `lax.all_gather`/`lax.psum` ç­‰ |

<h3 id="auto-sharding-mode">è‡ªåŠ¨æŒ¡ï¼šè®©ç¼–è¯‘å™¨å¸®ä½ æå®š</h3>

> **æ ¸å¿ƒæ€æƒ³**ï¼šä½ å†™æ­£å¸¸çš„ JAX ä»£ç ï¼Œå‘Šè¯‰ JAX è¾“å…¥è¾“å‡ºæ€ä¹ˆåˆ†ç‰‡ï¼Œå‰©ä¸‹çš„äº¤ç»™ XLA ç¼–è¯‘å™¨ã€‚

`jax.jit` åœ¨ JAX é‡Œå…¶å®å¹²ä¸¤ä»¶äº‹ï¼š
1. **JIT ç¼–è¯‘**ï¼šæŠŠ Python å‡½æ•°ç¼–è¯‘æˆé«˜æ•ˆçš„æœºå™¨ç 
2. **è‡ªåŠ¨å¹¶è¡Œ**ï¼šå¦‚æœè¾“å…¥æ˜¯åˆ†ç‰‡çš„ï¼Œè‡ªåŠ¨åœ¨å¤šè®¾å¤‡é—´åˆ†å‘è®¡ç®—

**æ¥çœ‹ä¸ªä¾‹å­**â€”â€”åˆ†ç‰‡çŸ©é˜µä¹˜æ³•ï¼š

```py
import jax
import jax.numpy as jnp

# å‡è®¾åœ¨ TPU v5e 4x2 ä¸Šè·‘ï¼Œ8 ä¸ªèŠ¯ç‰‡æ’æˆ 4 è¡Œ 2 åˆ—
mesh = jax.make_mesh(axis_shapes=(4, 2), axis_names=('X', 'Y'))

# å‘Šè¯‰ JAX åé¢éƒ½ç”¨è¿™ä¸ª mesh
jax.set_mesh(mesh)

# åˆ›å»ºåˆ†ç‰‡çš„è¾“å…¥å’Œæƒé‡
# In: [8, 2048] æ²¿ X åˆ‡ 4 ä»½ï¼ˆè¡Œï¼‰ï¼Œæ²¿ Y åˆ‡ 2 ä»½ï¼ˆåˆ—ï¼‰
# W: [2048, 8192] åªæ²¿ Y åˆ‡ 2 ä»½ï¼ˆè¡Œï¼‰ï¼Œåˆ—æ–¹å‘ä¸åˆ‡
In = jnp.zeros((8, 2048), dtype=jnp.bfloat16, device=jax.NamedSharding(mesh, jax.P('X', 'Y')))
W = jnp.zeros((2048, 8192), dtype=jnp.bfloat16, device=jax.NamedSharding(mesh, jax.P('Y', None)))

def matmul_square(In, W):
  return jnp.einsum('bd,df->bf', jnp.square(In), W)

# ç¼–è¯‘ï¼æŒ‡å®šè¾“å‡ºåˆ†ç‰‡ä¸º P('X', None) è¡¨ç¤ºè¡Œåˆ‡ X ä»½ï¼Œåˆ—ä¸åˆ‡ï¼ˆå¤åˆ¶ï¼‰
jit_matmul = jax.jit(matmul_square, out_shardings=jax.P('X', None)).lower(In, W).compile()

out = jit_matmul(In, W)
```

**åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**

è®©æˆ‘ä»¬ç”¨ä¹‹å‰å­¦è¿‡çš„ç¬¦å·æ¥ç†è§£ï¼š

| å¼ é‡ | å…¨å±€å½¢çŠ¶ | åˆ†ç‰‡ | æ¯ä¸ªè®¾å¤‡ä¸Šçš„å½¢çŠ¶ |
|:---|:---|:---|:---|
| In | [8, 2048] | [B<sub>X</sub>, D<sub>Y</sub>] | [2, 1024] |
| W | [2048, 8192] | [D<sub>Y</sub>, F] | [1024, 8192] |
| Out | [8, 8192] | [B<sub>X</sub>, F] å¤åˆ¶ | [2, 8192] |

å› ä¸º In å’Œ W åœ¨æ”¶ç¼©ç»´åº¦ D ä¸Šéƒ½è¢« Y è½´åˆ‡åˆ†äº†ï¼Œæœ¬åœ° matmul å¾—åˆ°çš„æ˜¯**éƒ¨åˆ†å’Œ**ï¼Œéœ€è¦ AllReduce æ‰èƒ½å¾—åˆ°å®Œæ•´ç»“æœï¼š

```
1. Out[B_X, F] { éƒ¨åˆ†å’Œ } = In[B_X, D_Y] Ã— W[D_Y, F]   # æœ¬åœ° matmul
2. Out[B_X, F] = AllReduce(Out[B_X, F])                # è·¨ Y è½´æ±‚å’Œ
```

ç”¨ `jit_matmul.as_text()` å¯ä»¥çœ‹åˆ°ç”Ÿæˆçš„ HLOï¼š

```py
# matmul èåˆæ“ä½œ
%fusion = bf16[2,8192] fusion(bf16[2,1024] %param, bf16[8192,1024] %copy-done)

# AllReduce æ±‚å’Œ
ROOT %AllReduce = bf16[2,8192] AllReduce(bf16[2,8192] %fusion)
```

æ³¨æ„å½¢çŠ¶ï¼š`bf16[2, 1024]` æ˜¯æœ¬åœ°æ¿€æ´»ï¼ˆå…¨å±€ 8 è¢« 4 åˆ‡åˆ†æˆ 2ï¼Œå…¨å±€ 2048 è¢« 2 åˆ‡åˆ†æˆ 1024ï¼‰ã€‚

**è¿™å°±æ˜¯ magicï¼** ä¸ç®¡ä½ çš„ç¨‹åºå¤šå¤æ‚ï¼Œ[Shardy](https://openxla.org/shardy) éƒ½ä¼šå°è¯•ï¼š
- ä¸ºæ‰€æœ‰ä¸­é—´æ¿€æ´»æ‰¾åˆ°åˆé€‚çš„åˆ†ç‰‡
- è‡ªåŠ¨æ’å…¥å¿…è¦çš„é€šä¿¡æ“ä½œ

**ä½†ç¼–è¯‘å™¨æœ‰æ—¶ä¼š"æŠ½é£"**

Shardy ä¸æ˜¯å®Œç¾çš„ã€‚æœ‰æ—¶ä½ æ‰“å¼€ profile ä¸€çœ‹â€”â€”æˆ‘æ“¦ï¼Œä¸€ä¸ªå·¨å¤§çš„ AllGather å äº† 80% çš„æ—¶é—´ï¼Œä½†å…¶å®æ ¹æœ¬ä¸éœ€è¦ï¼

è¿™æ—¶å€™å¯ä»¥ç”¨ `jax.lax.with_sharding_constraint` æ¥"çº æ­£"ç¼–è¯‘å™¨ï¼š

```py
import jax
import jax.numpy as jnp

mesh = jax.make_mesh((4, 2), ('X', 'Y'))

def matmul(x, Win, Wout):
  hidden = jnp.einsum('bd,df->bf', x, Win)
  # å¼ºåˆ¶ hidden æ²¿ y ç»´åº¦åˆ†ç‰‡ï¼ˆç¼–è¯‘å™¨æœ¬æ¥å¯èƒ½é€‰åˆ«çš„åˆ†ç‰‡ï¼‰
  hidden = jax.lax.with_sharding_constraint(hidden, jax.P('x', 'y'))
  return jnp.einsum('bf,df->bd', hidden, Wout)
```

**è‡ªåŠ¨æ¨¡å¼çš„ç—›ç‚¹**ï¼š"è°ƒæ•™ç¼–è¯‘å™¨"æ˜¯ä¸ªç„å­¦æ´»å„¿ã€‚ä½ å¯ä»¥æ ‡æ³¨æ¯ä¸ªä¸­é—´å˜é‡çš„åˆ†ç‰‡ï¼Œä½†è¿˜æ˜¯ä¸ç¡®å®šæœ€ç»ˆä¼šä¸ä¼šå¾—åˆ°æƒ³è¦çš„ç»“æœã€‚èƒ½ä¸èƒ½è®© JAX è‡ªå·±ç®¡åˆ†ç‰‡ä¼ æ’­å‘¢ï¼Ÿ

<h3 id="explicit-sharding-mode">åŠè‡ªåŠ¨æŒ¡ï¼šJAX æ§åˆ¶åˆ†ç‰‡ä¼ æ’­</h3>

> **æ ¸å¿ƒæ€æƒ³**ï¼šåˆ†ç‰‡ä¿¡æ¯å˜æˆç±»å‹ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ã€‚JAX ä¼šè¿½è¸ªæ¯ä¸ªæ“ä½œçš„åˆ†ç‰‡ï¼Œé‡åˆ°æ­§ä¹‰å°±æŠ¥é”™è®©ä½ æ˜ç¡®ã€‚

æ˜¾å¼åˆ†ç‰‡ï¼ˆExplicit Shardingï¼‰åˆå«"ç±»å‹ä¸­çš„åˆ†ç‰‡"â€”â€”åˆ†ç‰‡ä¼ æ’­åœ¨ JAX å±‚é¢å®Œæˆï¼Œè€Œä¸æ˜¯äº¤ç»™ XLAã€‚

**çœ‹ä¸ªä¾‹å­**ï¼š

```py
import jax
import jax.numpy as jnp
import jax.sharding as shd

# åˆ›å»º 2x2 meshï¼Œæ³¨æ„ axis_types æ˜¯ Explicit
mesh = jax.make_mesh(axis_shapes=(2, 2), axis_names=('X', 'Y'),
                                       axis_types=(shd.AxisType.Explicit, shd.AxisType.Explicit))
jax.set_mesh(mesh)

x = jax.device_put(np.arange(16).reshape(8, 2), jax.P('X', 'Y'))

@jax.jit
def f(x):
  print(jax.typeof(x))  # bfloat16[8@X,2@Y] â† åˆ†ç‰‡ä¿¡æ¯ç›´æ¥åœ¨ç±»å‹é‡Œï¼
  out = x * 2
  print(jax.typeof(out))  # bfloat16[8@X,2@Y] â† é€å…ƒç´ æ“ä½œä¿æŒåˆ†ç‰‡
  return out

f(x)
```

**JAX æ€ä¹ˆä¼ æ’­åˆ†ç‰‡ï¼Ÿ**

æ¯ä¸ª JAX æ“ä½œéƒ½æœ‰åˆ†ç‰‡è§„åˆ™ï¼š
- **é€å…ƒç´ æ“ä½œ**ï¼ˆåŠ å‡ä¹˜é™¤ï¼‰ï¼šè¾“å‡ºåˆ†ç‰‡ = è¾“å…¥åˆ†ç‰‡ï¼ˆå¾ˆæ˜¾ç„¶ï¼‰
- **è§„çº¦æ“ä½œ**ï¼ˆsumã€meanï¼‰ï¼šå¯èƒ½æ”¹å˜åˆ†ç‰‡
- **çŸ©é˜µä¹˜æ³•**ï¼šå¯èƒ½æœ‰æ­§ä¹‰ï¼Œéœ€è¦ä½ æ˜ç¡®

**æ­§ä¹‰æƒ…å†µâ€”â€”JAX ä¼šæŠ¥é”™**ï¼š

```py
# åˆ›å»ºåˆ†ç‰‡çš„è¾“å…¥å’Œæƒé‡
In = jnp.zeros((8, 2048), dtype=jnp.bfloat16, out_sharding=jax.P('X', 'Y'))
W = jnp.zeros((2048, 8192), dtype=jnp.bfloat16, out_sharding=jax.P('Y', None))

@jax.jit
def matmul_square(In, W):
  print(jax.typeof(In))  # bfloat16[8@X, 2048@Y]
  print(jax.typeof(W))   # bfloat16[2048@Y, 8192]
  return jnp.einsum('bd,df->bf', jnp.square(In), W)

matmul_square(In, W)  # ğŸ’¥ æŠ¥é”™ï¼
```

æŠ¥é”™ä¿¡æ¯å¾ˆæ¸…æ¥šï¼š

```
Contracting dimensions are sharded and it is ambiguous how the output should be sharded.
æ”¶ç¼©ç»´åº¦è¢«åˆ†ç‰‡äº†ï¼Œè¾“å‡ºåˆ†ç‰‡ä¸æ˜ç¡®ã€‚è¯·ç”¨ out_sharding å‚æ•°æŒ‡å®šã€‚
```

**ä¸ºä»€ä¹ˆæœ‰æ­§ä¹‰ï¼Ÿ** å› ä¸ºè¾“å‡ºå¯ä»¥æ˜¯ï¼š
- `P('X', 'Y')` â†’ è§¦å‘ ReduceScatter
- `P('X', None)` â†’ è§¦å‘ AllReduce

è‡ªåŠ¨æ¨¡å¼ä¼šéšä¾¿é€‰ä¸€ä¸ªï¼ˆå¯èƒ½é€‰é”™ï¼‰ï¼Œæ˜¾å¼æ¨¡å¼è®©ä½ è‡ªå·±å†³å®šï¼š

```py
@jax.jit
def matmul_square(In, W):
  # æ˜ç¡®å‘Šè¯‰ JAXï¼šæˆ‘è¦è¾“å‡ºæ²¿ X å’Œ Y éƒ½åˆ†ç‰‡
  return jnp.einsum('bd,df->bf', jnp.square(In), W, out_sharding=jax.P('X', 'Y'))

out = matmul_square(In, W)
print(jax.typeof(out))  # bfloat16[8@X,8192@Y]
```

**è‡ªåŠ¨ vs æ˜¾å¼å¯ä»¥æ··ç”¨**

é€šè¿‡ `jax.sharding.auto_axes` å’Œ `jax.sharding.explicit_axes` å¯ä»¥åœ¨åŒä¸€ç¨‹åºé‡Œæ··åˆä½¿ç”¨ã€‚è¯¦è§[å®˜æ–¹æ–‡æ¡£](https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html)ã€‚

<h3 id="manual-sharding-mode-via-shard_map">æ‰‹åŠ¨æŒ¡ï¼šshard_map å…¨æ‰‹å†™</h3>

> **æ ¸å¿ƒæ€æƒ³**ï¼šä½ æ‹¿åˆ°æ¯ä¸ªè®¾å¤‡çš„æœ¬åœ°è§†å›¾ï¼Œæ‰€æœ‰é€šä¿¡è‡ªå·±ç”¨ `lax` åŸè¯­å†™ã€‚å®Œå…¨æŒæ§ï¼Œä½†ä¹Ÿå®Œå…¨è´Ÿè´£ã€‚

**jax.jit vs jax.shard_map**ï¼š

| å¯¹æ¯” | jax.jit | jax.shard_map |
|:---|:---|:---|
| ä½ çœ‹åˆ°çš„ | å…¨å±€æ•°ç»„ | æœ¬åœ°åˆ†ç‰‡ |
| é€šä¿¡ | ç¼–è¯‘å™¨è‡ªåŠ¨åŠ  | ä½ æ‰‹åŠ¨å†™ |
| æ§åˆ¶åŠ› | ä½ | é«˜ |
| éš¾åº¦ | ç®€å• | å›°éš¾ |

**çœ‹ä¸ªä¾‹å­**â€”â€”åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šå–å‰ 4 ä¸ªå…ƒç´ ï¼Œç„¶åå…¨å±€å¹³å‡ï¼š<d-footnote>æ²¡æœ‰ TPUï¼Ÿç”¨è¿™è¡Œæ¨¡æ‹Ÿï¼š`import jax; jax.config.update('jax_num_cpu_devices', 8)`</d-footnote>

```py
import jax
import jax.numpy as jnp
import jax.sharding as shd

mesh = jax.make_mesh((2, 4), ('x', 'y'), (shd.AxisType.Explicit, shd.AxisType.Explicit))
jax.set_mesh(mesh)

x = jnp.arange(0, 512, dtype=jnp.int32, out_sharding=jax.P(('x', 'y')))

# è¿™ä¸ªå‡½æ•°åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šåªçœ‹åˆ° 1/8 çš„æ•°æ®ï¼
@jax.shard_map(in_specs=jax.P(('x', 'y')), out_specs=jax.P())
def slice_and_average(x):
  assert x.shape == (512 // 8,)  # æ¯ä¸ªè®¾å¤‡åªæœ‰ 64 ä¸ªå…ƒç´ 
  return jax.lax.pmean(x[:4], axis_name=('x', 'y'))  # æ‰‹åŠ¨å†™é€šä¿¡ï¼

out = slice_and_average(x)
assert out.shape == (4,)
```

**è¿™ä»£ç å¹²äº†å•¥ï¼Ÿ**

1. å…¨å±€æ•°ç»„ `x` æœ‰ 512 ä¸ªå…ƒç´ 
2. è¢« 8 ä¸ªè®¾å¤‡ï¼ˆ2Ã—4ï¼‰åˆ†ç‰‡ï¼Œæ¯ä¸ªè®¾å¤‡åªçœ‹åˆ° 64 ä¸ªå…ƒç´ 
3. æ¯ä¸ªè®¾å¤‡å–è‡ªå·±é‚£ä»½çš„å‰ 4 ä¸ªå…ƒç´ 
4. `pmean` æ˜¯æ‰‹åŠ¨çš„ AllReduceï¼Œå¯¹æ‰€æœ‰è®¾å¤‡çš„è¿™ 4 ä¸ªå…ƒç´ æ±‚å¹³å‡

å®é™…æ•ˆæœï¼š`mean(x[:4], x[64:68], x[128:132], ...)`

**ä¸ºä»€ä¹ˆä¸ç”¨ jax.jitï¼Ÿ**

ç”¨ jax.jitï¼Œä½ çœ‹åˆ°çš„æ˜¯å…¨å±€çš„ `[512]` æ•°ç»„ï¼Œè¦åˆ‡å‡ºè¿™ç§"æ¯ 64 ä¸ªå–å‰ 4 ä¸ª"çš„æ¨¡å¼å¾ˆåˆ«æ‰­ï¼Œè€Œä¸”ç¼–è¯‘å™¨å¯èƒ½åŠ é”™é€šä¿¡ã€‚ç”¨ shard_mapï¼Œä½ ç›´æ¥æ“ä½œæœ¬åœ°æ•°æ®ï¼Œéœ€è¦ä»€ä¹ˆé€šä¿¡è‡ªå·±åŠ ã€‚

---

### å®æˆ˜ï¼šCollective Matmulï¼ˆé€šä¿¡è®¡ç®—é‡å ï¼‰

è¿™æ˜¯ shard_map æœ€ç»å…¸çš„åº”ç”¨åœºæ™¯ã€‚

**é—®é¢˜èƒŒæ™¯**ï¼šæ¨¡å‹å¹¶è¡Œæ—¶ï¼Œæ¿€æ´»æ˜¯åˆ†ç‰‡çš„ï¼š

```
A[B_X, D_Y] Ã— W[D, F_Y] â†’ Out[B_X, F_Y]
```

**æœ´ç´ åšæ³•**â€”â€”å…ˆ AllGatherï¼Œå† matmulï¼š

```
1. A[B_X, D] = AllGather_Y(A[B_X, D_Y])   # å…ˆæ”¶é›†å®Œæ•´æ¿€æ´»
2. Out[B_X, F_Y] = A[B_X, D] Ã— W[D, F_Y]  # å†åš matmul
```

**é—®é¢˜**ï¼šé€šä¿¡å’Œè®¡ç®—å®Œå…¨ä¸²è¡Œï¼Œæ•ˆç‡ä½ä¸‹ï¼

**Collective Matmul**â€”â€”è¾¹é€šä¿¡è¾¹è®¡ç®—ï¼ˆå‚è€ƒ [Wang et al. 2023](https://dl.acm.org/doi/pdf/10.1145/3567955.3567959)ï¼‰ï¼š

æ ¸å¿ƒæ€æƒ³ï¼š
1. æŠŠ W æŒ‰ Y è½´åˆ†æˆè‹¥å¹²å—
2. æ¯ä¸€æ­¥ï¼šåšä¸€å— matmul + æŠŠ A å¾€ä¸‹ä¸€ä¸ªè®¾å¤‡ä¼ 
3. é€šä¿¡å’Œè®¡ç®—å®Œç¾é‡å ï¼

```py
import functools

import jax
import jax.numpy as jnp
import jax.sharding as shd
import numpy as np

# TPU v5e-8 æˆ–ç”¨ jax.config.update('jax_num_cpu_devices', 8) æ¨¡æ‹Ÿ
mesh = jax.make_mesh(axis_shapes=(2, 4), axis_names=('X', 'Y'),
                                       axis_types=(shd.AxisType.Explicit, shd.AxisType.Explicit))
jax.set_mesh(mesh)

B, D, F = 1024, 2048, 8192
A = jnp.arange(np.prod((B, D))).reshape((B, D))
W = jnp.arange(np.prod((D, F))).reshape((D, F))

A = jax.device_put(A, jax.P('X', 'Y'))
W = jax.device_put(W, jax.P(None, 'Y'))

@functools.partial(jax.jit, out_shardings=jax.P('X', 'Y'))
def matmul(lhs, rhs):
  return lhs @ rhs

def collective_matmul_allgather_lhs_contracting(lhs, rhs):
  """è¾¹ä¼ æ•°æ®è¾¹ç®—ï¼Œé€šä¿¡è®¡ç®—é‡å """
  axis_size = jax.lax.axis_size('Y')  # Y è½´æœ‰ 4 ä¸ªè®¾å¤‡
  idx = jax.lax.axis_index('Y')        # å½“å‰è®¾å¤‡åœ¨ Y è½´çš„ä½ç½®

  chunk_size = lhs.shape[1]
  assert rhs.shape[0] % chunk_size == 0

  def f(i, carrys):
    accum, lhs = carrys
    # ä» W ä¸­å–å‡ºå¯¹åº”å—
    rhs_chunk = jax.lax.dynamic_slice_in_dim(rhs, (idx + i) % axis_size * chunk_size, chunk_size)
    # æœ¬åœ° matmul
    update = lhs @ rhs_chunk
    # æŠŠ lhs å‘å·¦å¾ªç¯ç§»ä½ï¼ˆä¼ ç»™ç›¸é‚»è®¾å¤‡ï¼‰
    lhs = jax.lax.ppermute(
        lhs,
        axis_name='Y',
        perm=[(j, (j - 1) % axis_size) for j in range(axis_size)]
    )
    return accum + update, lhs

  accum = jnp.zeros((lhs.shape[0], rhs.shape[1]), dtype=lhs.dtype)
  accum = jax.lax.pvary(accum, ('X', 'Y'))
  accum, lhs = jax.lax.fori_loop(0, axis_size - 1, f, (accum, lhs), unroll=True)

  # å¤„ç†æœ€åä¸€å—
  i = axis_size - 1
  rhs_chunk = jax.lax.dynamic_slice_in_dim(rhs, (idx + i) % axis_size * chunk_size, chunk_size)
  update = lhs @ rhs_chunk
  return accum + update

jit_sharded_f = jax.jit(jax.shard_map(
  collective_matmul_allgather_lhs_contracting,
  in_specs=(jax.P('X', 'Y'), jax.P(None, 'Y')), out_specs=jax.P('X', 'Y')))

shmapped_out = jit_sharded_f(A, W)
expected_out = matmul(A, W)

np.testing.assert_array_equal(shmapped_out, expected_out)
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| ç‰ˆæœ¬ | è€—æ—¶ | Profile ç‰¹å¾ |
|:---|:---:|:---|
| æœ´ç´  jit matmul | 311us | å¼€å¤´æœ‰å¤§å—é˜»å¡ AllGather |
| Collective matmul | 244us | æ²¡æœ‰ç‹¬ç«‹é€šä¿¡ï¼Œå…¨æ˜¯æœ‰ç”¨è®¡ç®— |
| æ— åˆ†ç‰‡åŸºçº¿ | 224us | çº¯ matmul |

{% include figure.liquid path="assets/img/not-overlapped.png" class="img-fluid" %}
<center><i>æœ´ç´ ç‰ˆæœ¬ï¼šå¼€å¤´çš„å¤§å—è“è‰²æ˜¯ AllGatherï¼Œè®¡ç®—åœ¨ç­‰é€šä¿¡</i></center>

{% include figure.liquid path="assets/img/overlapped.png" class="img-fluid" %}
<center><i>Collective ç‰ˆæœ¬ï¼šæ²¡æœ‰ç‹¬ç«‹é€šä¿¡ï¼ŒFLOPs åˆ©ç”¨ç‡æš´æ¶¨</i></center>

æˆ‘ä»¬è¾¾åˆ°äº†æ¥è¿‘æ— åˆ†ç‰‡åŸºçº¿çš„æ€§èƒ½ï¼è¿™å°±æ˜¯æ€§èƒ½ä¼˜åŒ–çš„å¨åŠ›ã€‚æ›´å¤š shard_map ä¾‹å­è§[å®˜æ–¹ç¬”è®°](https://jax.readthedocs.io/en/latest/notebooks/shard_map.html#example-1-all-gather-on-one-side)ã€‚

---

## ç»ƒä¹ é¢˜

> **å‡†å¤‡å·¥ä½œ**ï¼šè¿™äº›é¢˜éœ€è¦å¤šä¸ª TPUã€‚å¯ä»¥ç”¨å…è´¹çš„ Colab TPUv2-8ï¼Œæˆ–è€…ç”¨ `jax.config.update('jax_num_cpu_devices', 8)` æ¨¡æ‹Ÿã€‚

---

### é—®é¢˜ 1ï¼šåˆ†ç‰‡å†…å¹³å‡ & åˆ†ç‰‡å†… Roll

è®¾ **A** æ˜¯å½¢çŠ¶ä¸º `float32[S_X, D_Y]` çš„æ•°ç»„ï¼Œè¢«åˆ†ç‰‡åˆ° N = X Ã— Y ä¸ªè®¾å¤‡ä¸Šã€‚

**(a) åˆ†ç‰‡å†…å¹³å‡**

å†™ä¸€ä¸ªå‡½æ•°ï¼šè¿”å›å½¢çŠ¶ `[X, Y]` çš„æ•°ç»„ï¼Œå…¶ä¸­ `arr[i, j]` æ˜¯åˆ†ç‰‡ `(i, j)` ä¸Šæ•°æ®çš„å¹³å‡å€¼ã€‚

è¦æ±‚ï¼š
- åˆ†åˆ«ç”¨ `jax.jit` å’Œ `shard_map` å®ç°
- ç”¨ profiler çœ‹çœ‹è€—æ—¶
- æ£€æŸ¥æœ‰æ²¡æœ‰ä¸å¿…è¦çš„é€šä¿¡ï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼ï¼‰

**(b) åˆ†ç‰‡å†… Roll**

å†™ä¸€ä¸ªå‡½æ•°ï¼šåœ¨æ¯ä¸ª X åˆ†ç‰‡å†…åš `roll(x, shift, axis=0) - x`ã€‚

åªéœ€è¦ç”¨ `shard_map` å®ç°ï¼ˆjit ç‰ˆæœ¬å¤ªæŠ˜ç£¨äººäº†ï¼‰ã€‚

{% details ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ %}

**(a) åˆ†ç‰‡å†…å¹³å‡**

æ³¨æ„ jit ç‰ˆæœ¬éœ€è¦åšå¤æ‚çš„ reshapeï¼š

```py
import numpy as np
import jax
import jax.numpy as jnp

mesh = jax.make_mesh((4, 2), ('X','Y'))

# shard_map ç‰ˆæœ¬ï¼šç›´æ¥å¯¹æœ¬åœ°æ•°æ®æ±‚å¹³å‡
average_shmap = jax.shard_map(
    lambda x: x.mean(keepdims=True),
    mesh=mesh,
    in_specs=jax.P('X','Y'), out_specs=jax.P('X','Y')
)

# jit ç‰ˆæœ¬ï¼šéœ€è¦æ‰‹åŠ¨ reshape æ¥æ¨¡æ‹Ÿåˆ†ç‰‡
def average(x):
  X, Y = mesh.axis_sizes
  return x.reshape(X, x.shape[0] // X, Y, x.shape[1] // Y).mean(axis=(1, 3))

average_jit = jax.jit(average, out_shardings=jax.NamedSharding(mesh, jax.P('X','Y')))

# æµ‹è¯•
x = jnp.arange(8 * 64 * 8, dtype=jnp.int32).reshape(8 * 64, 8)
x = jax.device_put(x, jax.NamedSharding(mesh, jax.P('X','Y')))

y1 = average_shmap(x)
y2 = average_jit(x)

np.testing.assert_array_equal(y1, y2)
```

**(b) åˆ†ç‰‡å†… Roll**

```py
import numpy as np
import jax
import jax.numpy as jnp
import functools

P = jax.sharding.PartitionSpec
mesh = jax.make_mesh((4, 2), ('X','Y'))

# shard_map ç‰ˆæœ¬ï¼šç›´æ¥å¯¹æœ¬åœ°æ•°æ® roll
def shift_shmap(x, shift: int):
  shmapped = jax.shard_map(
      lambda x: jnp.roll(x, shift, axis=0),
      mesh=mesh,
      in_specs=jax.P('X','Y'), out_specs=jax.P('X','Y')
  )
  return shmapped(x)

# jit ç‰ˆæœ¬ï¼šreshape ååœ¨æ­£ç¡®ç»´åº¦ä¸Š roll
@functools.partial(jax.jit, static_argnames=['shift'], out_shardings=jax.NamedSharding(mesh, jax.P('X','Y')))
def shift_jit(x, shift: int):
  X, Y = mesh.axis_sizes
  reshaped = x.reshape(X, x.shape[0] // X, -1)
  return jnp.roll(reshaped, shift, axis=1).reshape(x.shape[0], x.shape[1])

# æµ‹è¯•
x = jnp.arange(8 * 64 * 8, dtype=jnp.int32).reshape(8 * 64, 8)
x = jax.device_put(x, jax.NamedSharding(mesh, jax.P('X','Y')))

y1 = shift_shmap(x, 5)
y2 = shift_jit(x, 5)

np.testing.assert_array_equal(y1, y2)
```

{% enddetails %}

---

### é—®é¢˜ 2ï¼šæ··åˆä¸“å®¶ï¼ˆMoEï¼‰å®ç°

è¿™é¢˜ä¸€èµ·æ¥å®ç°ä¸€ä¸ªåŸºç¡€çš„ MoE å±‚ã€‚

**è®¾å®š**ï¼š
- **W**: `float32[E_X, D, F]` â€” E ä¸ªä¸“å®¶çŸ©é˜µ
- **A**: `float32[S_X, D]` â€” è¾“å…¥æ¿€æ´»
- **B**: `int32[S_X]` â€” è·¯ç”±åˆ†é…ï¼Œ`B[i]` å‘Šè¯‰æˆ‘ä»¬ç¬¬ i ä¸ª token è¯¥ç”¨å“ªä¸ªä¸“å®¶

**ç›®æ ‡**ï¼šè¿”å› `Out[i] = W[B[i]] @ A[i]`

**(a) æœ¬åœ°å®ç°**

å…ˆå¿½ç•¥åˆ†ç‰‡ï¼Œåœ¨å•è®¾å¤‡ä¸Šå®ç°ã€‚

âš ï¸ **ä¸è¦**å…·ä½“åŒ– `[S, D, F]` å½¢çŠ¶çš„æ•°ç»„ï¼

æç¤ºï¼šæŠŠ token æ’åºåˆ° `[E, S, D]` ç¼“å†²åŒºï¼Œç”¨ mask å¤„ç†ã€‚

**(b) ç›´æ¥ jit**

ç”¨ `jax.jit` åŒ…è£…ä½ çš„å®ç°ï¼Œprofile çœ‹çœ‹ç¼–è¯‘å™¨åŠ äº†ä»€ä¹ˆé€šä¿¡ï¼Œè€—æ—¶å¤šå°‘ï¼Ÿ

**(c) shard_map å®ç°**

ä½ ä¼šå‘ç° jit ç‰ˆæœ¬å¯èƒ½åœ¨æœ¬åœ° AllGather å®Œæ•´æ¿€æ´» Aï¼Œé€šä¿¡å’Œå†…å­˜éƒ½å¾ˆè´µã€‚ç”¨ `shard_map` é‡å†™ï¼š

1. ç¬¬ä¸€æ­¥ï¼šç”¨ `jax.lax.all_gather` æ”¶é›†åé‡æ’åº
2. è¿›é˜¶ï¼šé¿å…å…·ä½“åŒ– `[E, S, D]` æ•°ç»„ï¼Œç”¨ `jax.lax.while_loop` + `jax.lax.all_to_all` åšä¸è§„åˆ™è®¡ç®—

æ¯”åŸå§‹ç‰ˆæœ¬å¿«å¤šå°‘ï¼Ÿ

**(d) Top-K è·¯ç”±**

å¤§å¤šæ•° MoE è·¯ç”±åˆ°å¤šä¸ªä¸“å®¶å†å¹³å‡ã€‚è®© **B**: `int32[S, k]`ï¼Œå®ç° top-k è·¯ç”±ã€‚

{% details ç‚¹å‡»æŸ¥çœ‹éƒ¨åˆ†ç­”æ¡ˆ %}

**(a/b) æœ¬åœ°å®ç°**

æœ‰å¾ˆå¤šæ–¹æ³•ï¼Œè¿™æ˜¯ç”¨ mask è¿­ä»£ä¸“å®¶çš„ç‰ˆæœ¬ï¼š

```py
def moe_local(W: jnp.ndarray, A: jnp.ndarray, B: jnp.ndarray) -> jnp.ndarray:
    S, _ = A.shape
    E, _, F = W.shape

    def expert_forward(carry, e):
        output = carry  # [S, F]
        mask = (B == e)[:, None]  # [S, 1]
        expert_result = A @ W[e]  # [S, F] - è¿™ä¸ªä¸“å®¶å¯¹æ‰€æœ‰ token çš„å˜æ¢
        output = output + expert_result * mask  # åªä¿ç•™åˆ†é…çš„ token
        return output, None

    output = jnp.zeros((S, F))
    output, _ = lax.scan(expert_forward, output, jnp.arange(E))

    return output
```

ä½ ä¹Ÿå¯ä»¥ç”¨ `jax.lax.ragged_dot`ï¼Œæ›´é«˜æ•ˆã€‚

**(c) shard_map ä¼ªä»£ç **

```py
chunk_size = 128
def matmul(W, x, B):
  i = 0
  x = # æ ¹æ®åˆ†é…æ’åº x
  while (chunk := x[i:i+chunk_size].any()):
     chunk = all_to_all(chunk)
     out = matmul_local(W, chunk)
  return concat(out)
```

æ ¸å¿ƒæ€æƒ³ï¼šè¿­ä»£æ•°ç»„çš„å—ï¼Œæ’åº + all_to_allï¼Œç„¶ååšæœ¬åœ° FLOPsã€‚

{% enddetails %}

---

### é—®é¢˜ 3ï¼šCollective Matmul è¿›é˜¶

ä¸Šé¢çš„ collective matmul ä¾‹å­å¯¹çœŸå® LLM éå¸¸æœ‰ç”¨ã€‚æ¥æ‰©å±•ä¸€ä¸‹ï¼š

**(a) AllReduce Collective Matmul**

å®ç°ï¼š`A[B_X, D_Y] Ã—_D W[D_Y, F] â†’ Out[B_X, F]`

æœ´ç´ ç‰ˆæœ¬æ˜¯æœ¬åœ° matmul + AllReduceã€‚å®ç°é€šä¿¡é‡å ç‰ˆæœ¬ã€‚

æç¤ºï¼šåœ¨è¾“å‡ºç»´åº¦ä¸Šåˆ†å—ï¼Œè¾¹ç®—è¾¹ `jax.lax.psum`ã€‚

æ³¨æ„ï¼šç”±äº XLA ä¼˜åŒ–ï¼Œå¯èƒ½ä¸æ¯”åŸºçº¿å¿«ã€‚

**(b) ReduceScatter Collective Matmul**

è¿™æ˜¯ AllReduce çš„è¡¥å……ï¼Œå‘ç”Ÿåœ¨ Transformer çš„ down-projectionï¼š

`Tmp[B_X, F_Y] Ã—_F W2[F_Y, D] â†’ Out[B_X, D_Y]`

å®ç°é€šä¿¡é‡å ç‰ˆæœ¬ï¼Œåªä¼ å¿…è¦çš„æ•°æ®é‡ã€‚

æç¤ºï¼šç´¯ç§¯æ—¶ç½®æ¢ç»“æœã€‚

**(c) ç«¯åˆ°ç«¯ Transformer å—**

æŠŠ (a) å’Œ (b) ç»„åˆèµ·æ¥ï¼š

`In[B_X, D_Y] Ã—_D W_in[D, F_Y] Ã—_F W_out[F_Y, D] â†’ Out[B_X, D_Y]`

<d-footnote>è®°ä½ï¼šä¸èƒ½å…ˆç®— W_in Â· W_outï¼Œå› ä¸ºä¸­é—´æœ‰éçº¿æ€§ï¼</d-footnote>

æ¯” jit ç‰ˆæœ¬å¿«å¤šå°‘ï¼Ÿ

---

### é—®é¢˜ 4ï¼šåŒå‘é€šä¿¡ä¼˜åŒ–

ä¸Šé¢çš„ collective matmul éƒ½æ˜¯å•å‘ç½®æ¢ã€‚æ”¹æˆåŒå‘é€šä¿¡ã€‚

- é‡å†™ AllReduce collective matmul
- é‡å†™ ReduceScatter collective matmul

å¿«äº†å¤šå°‘ï¼Ÿ

---

### ğŸ‰ ç¬¬ 10 éƒ¨åˆ†å®Œç»“ï¼

åŸºæœ¬å†…å®¹å°±æ˜¯è¿™äº›äº†ã€‚è¦çœ‹æœ€ç»ˆæ€»ç»“å’Œè¿›ä¸€æ­¥é˜…è¯»ææ–™ï¼Œè¯·ç‚¹å‡»[è¿™é‡Œ](../conclusion)ã€‚
